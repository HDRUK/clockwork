#!/usr/bin/env python3

import clockwork
import argparse


parser = argparse.ArgumentParser(
    prog='clockwork',
    usage='clockwork <tasks> <options>',
    description='script to run CRyPTIC helper tasks',
)
subparsers = parser.add_subparsers(title='Available tasks', help='', metavar='')

#----------------------------- cortex --------------------------------
subparser_cortex = subparsers.add_parser(
    'cortex',
    help='Runs cortex',
    usage='clockwork cortex <reference files dir> <reads input file> <output dir> <sample name>',
    description='Runs cortex',
)

subparser_cortex.add_argument('--mem_height', type=int, help='cortex mem_height option [%(default)s]', default='22', metavar='INT')
subparser_cortex.add_argument('ref_dir', help='Directory containing reference files')
subparser_cortex.add_argument('reads_file', help='File containing reads')
subparser_cortex.add_argument('output_dir', help='Name of output directory')
subparser_cortex.add_argument('sample_name', help='Name of sample (is put into VCF files')
subparser_cortex.set_defaults(func=clockwork.tasks.cortex.run)


#----------------------- reference_prepare ---------------------------
subparser_reference_prepare = subparsers.add_parser(
    'reference_prepare',
    help='Makes reference genome files for use with pipelines',
    usage='clockwork reference_prepare [options] <fasta_file>',
    description='Makes reference genome files for use with pipelines',
    epilog='If adding to database, must use: --db_config_file, --pipeline_references_root, --name. Otherwise, must use --outdir.'
)

subparser_reference_prepare.add_argument('--cortex_mem_height', type=int, help='mem_height option for cortex [%(default)s]', metavar='INT', default=22)
subparser_reference_prepare.add_argument('--db_config_file', help='Name of database config file', metavar='FILENAME')
subparser_reference_prepare.add_argument('--pipeline_references_root', help='Root directory of pipeline references', metavar='DIRNAME')
subparser_reference_prepare.add_argument('--contam_tsv', help='Contamination metadata tsv file, if this reference is to be used for remove_contam pipeline', metavar='FILENAME')
subparser_reference_prepare.add_argument('--name', help='Name of reference', metavar='STR')
subparser_reference_prepare.add_argument('--outdir', help='Name of output directory (must not already exist)', metavar='DIRNAME')
subparser_reference_prepare.add_argument('fasta_file', help='FASTA filename')
subparser_reference_prepare.set_defaults(func=clockwork.tasks.reference_prepare.run)


#--------------------- db_finished_pipeline_update -------------------
subparser_db_finished_pipeline_update = subparsers.add_parser(
    'db_finished_pipeline_update',
    help='Update database after pipeline has been run',
    usage='clockwork db_finished_pipeline_update <db config file> <pool> <isolate_id> <seqrep_id> <seqrep_pool> <pipeline name>',
    description='Update database after pipeline has been run',
)

subparser_db_finished_pipeline_update.add_argument('--new_pipeline_status', type=int, help='Value of status column in Pipeline table [%(default)s]', default=1, metavar='INT')
subparser_db_finished_pipeline_update.add_argument('--pipeline_root', help='pipeline_root directory. Required if --new_pipeline_status != -1, and pipeline_name is "qc" or "remove_contam"')
subparser_db_finished_pipeline_update.add_argument('--pipeline_version', help='pipeline_version in database Pipeline table')
subparser_db_finished_pipeline_update.add_argument('--reference_id', type=int, help='Reference genome ID', metavar='INT')
subparser_db_finished_pipeline_update.add_argument('db_config_file', help='Name of database config file')
subparser_db_finished_pipeline_update.add_argument('pool', type=int, choices=[0, 1], help='0=not pooled and seqrep_pool is ignored, 1=pooled and seqrep_id is ignored')
subparser_db_finished_pipeline_update.add_argument('isolate_id', type=int, help='isolate_id in database Pipeline table')
subparser_db_finished_pipeline_update.add_argument('seqrep_id', help='seqrep_id in database Pipeline table')
subparser_db_finished_pipeline_update.add_argument('seqrep_pool', help='seqrep_pool in database Pipeline table')
subparser_db_finished_pipeline_update.add_argument('pipeline_name', help='pipeline_name in database Pipeline table')
subparser_db_finished_pipeline_update.set_defaults(func=clockwork.tasks.db_finished_pipeline_update.run)


#---------------- db_finished_pipeline_update_failed_jobs -----------
subparser_db_finished_pipeline_update_failed_jobs = subparsers.add_parser(
    'db_finished_pipeline_update_failed_jobs',
    help='Update database with failed jobs at end of pipeline',
    usage='clockwork db_finished_pipeline_update_failed_jobs [options] <db config file> <jobs.tsv> <success file> <pipeline name>',
    description='Update database with failed jobs after pipeline has been run',
)

subparser_db_finished_pipeline_update_failed_jobs.add_argument('--pipeline_version', help='pipeline_version in database Pipeline table')
subparser_db_finished_pipeline_update_failed_jobs.add_argument('--reference_id', type=int, help='Reference genome ID', metavar='INT')
subparser_db_finished_pipeline_update_failed_jobs.add_argument('db_config_file', help='Name of database config file')
subparser_db_finished_pipeline_update_failed_jobs.add_argument('jobs_tsv', help='Name of jobs TSV file made at start of piepline')
subparser_db_finished_pipeline_update_failed_jobs.add_argument('success_jobs_file', help='Name of successful jobs file made by pipeline')
subparser_db_finished_pipeline_update_failed_jobs.add_argument('pipeline_name', help='pipeline_name in database Pipeline table')
subparser_db_finished_pipeline_update_failed_jobs.set_defaults(func=clockwork.tasks.db_finished_pipeline_update_failed_jobs.run)


#----------------------------- ena_download -------------------------
subparser_ena_download = subparsers.add_parser(
    'ena_download',
    help='Download reads from the ENA',
    usage='clockwork ena_download [options] <data_tsv> <output_dir> <site> <lab> <date> <dataset name>',
    description='Download reads from ENA. Gets fastq files, and makes import spreadsheet for use with the import pipeline',
    epilog='The data TSV file is one line per sample, and each line needs two columns. Column 1 is the sample name, and can be anything unique within the file. Column 2 must be a comma-separated list of run accessions.'
)

subparser_ena_download.add_argument('--md5_threads', type=int, help='Number of files to calculate md5 in parallel [%(default)s]', default=1, metavar='INT')
subparser_ena_download.add_argument('--download_threads', type=int, help='Number of files to download in parallel [%(default)s]', default=1, metavar='INT')
subparser_ena_download.add_argument('data_tsv', help='Name of input TSV file with accessions. See below for description.')
subparser_ena_download.add_argument('output_dir', help='Name of output directory')
subparser_ena_download.add_argument('site', help='Every sample gets given this site_id in the database')
subparser_ena_download.add_argument('lab', help='Every sample gets given this isolate_number_from_lab in the database')
subparser_ena_download.add_argument('date', help='Date in YYYYMMDD format')
subparser_ena_download.add_argument('dataset_name', help='Dataset name')
subparser_ena_download.set_defaults(func=clockwork.tasks.ena_download.run)


#----------------------------- ena_submit_reads  -------------------------
subparser_ena_submit_reads = subparsers.add_parser(
    'ena_submit_reads',
    help='Submit reads to the ENA',
    usage='clockwork ena_submit_reads  [options] <ini_file> <dataset_name> <pipeline_root>',
    description='Submit all reads from a dataset to the ENA.',
)

subparser_ena_submit_reads.add_argument('--fq_upload_threads', type=int, help='Threads to use when uploading fastq files [%(default)s]', default=1, metavar='INT')
subparser_ena_submit_reads.add_argument('--use_test_server', action='store_true', help='Use test server instead of production')
subparser_ena_submit_reads.add_argument('--taxon_id', type=int, help='Taxon ID [%(default)s]', default=1494075, metavar='INT')
subparser_ena_submit_reads.add_argument('ini_file', help='Name of ini file that has database and ENA account logins etc')
subparser_ena_submit_reads.add_argument('dataset_name', help='Name of data dataset to be submitted')
subparser_ena_submit_reads.add_argument('pipeline_root', help='Pipeline root directory')
subparser_ena_submit_reads.set_defaults(func=clockwork.tasks.ena_submit_reads.run)


#----------------------------- make_empty_db -------------------------
subparser_make_empty_db = subparsers.add_parser(
    'make_empty_db',
    help='Make new database with empy tables',
    usage='clockwork make_empty_db <db_config_file>',
    description='Make new database with empy tables',
)

subparser_make_empty_db.add_argument('db_config_file', help='Name of database config file')
subparser_make_empty_db.set_defaults(func=clockwork.tasks.make_empty_db.run)


#----------------------------- fastqc --------------------------------
subparser_fastqc = subparsers.add_parser(
    'fastqc',
    help='Wrapper for FASTQC',
    usage='clockwork fastqc <output directory> <file1> [reads file2 [reads file3...]]',
    description='Wrapper for FASTQC',
)

subparser_fastqc.add_argument('output_dir', help='Name of output directory (must not already exist)')
subparser_fastqc.add_argument('reads_files', nargs='+', help='List of reads filenames (must provide at least one)')
subparser_fastqc.set_defaults(func=clockwork.tasks.fastqc.run)


#---------------- generic_pipeline_make_jobs_tsv -----------------------
subparser_generic_pipeline_make_jobs_tsv = subparsers.add_parser(
    'generic_pipeline_make_jobs_tsv',
    help='Finds which sample need running a generic pipeline',
    usage='clockwork generic_pipeline_make_jobs_tsv [options] <pipeline name> <pipeline root> <db config file> <outfile>',
    description='Writes TSV file of generic_pipeline jobs to be run, and updates the database. TSV file is meant to be used as part of nextflow pipeline generic_pipeline',
)

subparser_generic_pipeline_make_jobs_tsv.add_argument('--dataset_name', help='limit to the given dataset name', metavar='STR')
subparser_generic_pipeline_make_jobs_tsv.add_argument('--pipeline_version', help='Pipeline version to run')
subparser_generic_pipeline_make_jobs_tsv.add_argument('pipeline_name', help='Name of pipeilne')
subparser_generic_pipeline_make_jobs_tsv.add_argument('pipeline_root', help='Root directory of pipeline')
subparser_generic_pipeline_make_jobs_tsv.add_argument('db_config_file', help='Name of database config file')
subparser_generic_pipeline_make_jobs_tsv.add_argument('outfile', help='Name of output file')
subparser_generic_pipeline_make_jobs_tsv.set_defaults(func=clockwork.tasks.generic_pipeline_make_jobs_tsv.run)


#---------------------- import_read_pair -----------------------------
subparser_import_read_pair = subparsers.add_parser(
    'import_read_pair',
    help='Imports a read pair',
    usage='clockwork import_read_pair <database config file> <pipeline root> <seqrep_id> <isolate_id> <sample_id> <sequence_replicate_number> <reads file 1> <reads file 2> <reads MD5 1> <reads MD5 2>',
    description='rsyncs pair of FASTQ files into pipeline directory structure, updates database, deletes original FASTQ files',
)

subparser_import_read_pair.add_argument('db_config_file', help='Name of database config file')
subparser_import_read_pair.add_argument('pipeline_root', help='Root directory of pipeline')
subparser_import_read_pair.add_argument('seqrep_id', type=int, help='seqrep_id in Seqrep table of database')
subparser_import_read_pair.add_argument('isolate_id', type=int, help='isolate_id in Seqrep table of database')
subparser_import_read_pair.add_argument('sample_id', type=int, help='sample_id in Sample table of database')
subparser_import_read_pair.add_argument('sequence_rep_number', type=int, help='sequence_replicate_number in Seqrep table of database')
subparser_import_read_pair.add_argument('reads_file_1', help='Name of first reads file')
subparser_import_read_pair.add_argument('reads_file_2', help='Name of second reads file')
subparser_import_read_pair.add_argument('reads_file_md5_1', help='MD5 of first reads file')
subparser_import_read_pair.add_argument('reads_file_md5_2', help='MD5 of second reads file')
subparser_import_read_pair.set_defaults(func=clockwork.tasks.import_read_pair.run)


#---------------------- make_import_spreadsheet ----------------------
subparser_make_import_spreadsheet = subparsers.add_parser(
    'make_import_spreadsheet',
    help='Makes an import spreadsheet xlsx file, with just the column headings',
    usage='clockwork make_import_spreadsheet <out.xlsx>',
    description='Makes an import spreadsheet xlsx file, with just the column headings',
)

subparser_make_import_spreadsheet.add_argument('outfile', help='Name of output xlsx file')
subparser_make_import_spreadsheet.set_defaults(func=clockwork.tasks.make_import_spreadsheet.run)

#----------------------- validate spreadhseet ------------------------
subparser_validate_spreadsheet = subparsers.add_parser(
    'validate_spreadsheet',
    help='Validates import spreadsheet',
    usage='clockwork validate_spreadsheet <in.xlsx> <data_dir> <errors_out.txt>',
    description='Validates import spreadsheet, and checks files in the sheet are on disk, in the directory data_dir. Writes errors to errors_out.txt -- this will be empty if no errors found',
)

subparser_validate_spreadsheet.add_argument('infile', help='Name of input spreadsheet file')
subparser_validate_spreadsheet.add_argument('data_dir', help='Name of directory containing FASTQ files')
subparser_validate_spreadsheet.add_argument('outfile', help='Name of output file containing any errors found')
subparser_validate_spreadsheet.add_argument('--threads', type=int, help='Number of files to calculate md5 sum in parallel. Set to zero to skip md5 checks [%(default)s]', metavar='INT', default=1)
subparser_validate_spreadsheet.set_defaults(func=clockwork.tasks.validate_spreadsheet.run)


#---------------------- import_spreadsheet ---------------------------
subparser_import_spreadsheet = subparsers.add_parser(
    'import_spreadsheet',
    help='Parses import spreadsheet and makes output for import_read_pair',
    usage='clockwork import_spreadsheet <dropbox dir> <database config file> <xlsx file> <xlsx archive dir> <jobs outfile>',
    description='Parses import spreadsheet, sanity checks it, adds new samples to database, and makes output file that is input to next stage of nextflow pipeline import_read_pair',
)

subparser_import_spreadsheet.add_argument('--db_backup_dir', help='Name of database backup files directory')
subparser_import_spreadsheet.add_argument('dropbox_dir', help='Name of dropbox directory')
subparser_import_spreadsheet.add_argument('db_config_file', help='Name of database config file')
subparser_import_spreadsheet.add_argument('xlsx_file', help='Name of xlsx file. Must be in dropbox_dir')
subparser_import_spreadsheet.add_argument('xls_archive_dir', help='Name of spreadsheet archive directory')
subparser_import_spreadsheet.add_argument('jobs_outfile', help='Name of output jobs tsv file')
subparser_import_spreadsheet.set_defaults(func=clockwork.tasks.import_spreadsheet.run)


#---------------------------- map_reads ------------------------------
subparser_map_reads = subparsers.add_parser(
    'map_reads',
    help='Maps reads, removes duplicates, outputs sorted BAM file',
    usage='clockwork map_reads <sample name> <ref_fasta> <outfile> <fwd reads 1> <rev reads 1> [<fwd reads 2> <rev reads 2> ...]',
    description='Maps reads with BWA MEM, removes duplicates with samtools rmdup, outputs sorted BAM file',
)

subparser_map_reads.add_argument('--unsorted_sam', action='store_true', help='Make an unsorted sam file')
subparser_map_reads.add_argument('sample_name', help='Name of sample. This is put into the final VCF files')
subparser_map_reads.add_argument('ref_fasta', help='Name of reference FASTA file. Must be indexed with BWA')
subparser_map_reads.add_argument('outfile', help='Name of output file')
subparser_map_reads.add_argument('reads_files', nargs='+', help='List of pairs of fwd, rev reads filenames. Must be even number of files!')
subparser_map_reads.set_defaults(func=clockwork.tasks.map_reads.run)


#--------------------- qc_make_jobs_tsv ------------------------------
subparser_qc_make_jobs_tsv = subparsers.add_parser(
    'qc_make_jobs_tsv',
    help='Finds which samples need running qc pipeline',
    usage='clockwork qc_make_jobs_tsv <pipeline root> <db config file> <outfile> <reference ID> <references root>',
    description='Writes TSV file of qc jobs to be run, and updates the database. TSV file is meant to be used as part of nextflow pipeline qc',
)

subparser_qc_make_jobs_tsv.add_argument('--dataset_name', help='limit to the given dataset name', metavar='STR')
subparser_qc_make_jobs_tsv.add_argument('--pipeline_version', help='Pipeline version to run')
subparser_qc_make_jobs_tsv.add_argument('pipeline_root', help='Root directory of pipeline')
subparser_qc_make_jobs_tsv.add_argument('db_config_file', help='Name of db config file')
subparser_qc_make_jobs_tsv.add_argument('outfile', help='Name of output file')
subparser_qc_make_jobs_tsv.add_argument('reference_id', type=int, help='Reference genome database ID')
subparser_qc_make_jobs_tsv.add_argument('reference_root', help='Root directory of pipeline reference genomes')
subparser_qc_make_jobs_tsv.set_defaults(func=clockwork.tasks.qc_make_jobs_tsv.run)


#---------------------- remove_contam --------------------------------
subparser_remove_contam = subparsers.add_parser(
    'remove_contam',
    help='Remove contaminated reads from SAM or BAM',
    usage='clockwork remove_contam [options] <ref seq metadata tsv> <bam file> <counts outfile> <reads_out_1> <reads_out_2>',
    description='(SAM or BAM) -> paired FASTQ files split into OK, contaminated, and unmapped',
)

subparser_remove_contam.add_argument('--no_match_out_1', help='Name of output file 1 of reads that did not match. If not given, reads are included in reads_out_1. Must be used with --no_match_out_2')
subparser_remove_contam.add_argument('--no_match_out_2', help='Name of output file 2 of reads that did not match. If not given, reads are included in reads_out_2. Must be used with --no_match_out_1')
subparser_remove_contam.add_argument('--contam_out_1', help='Name of output file 1 of contamination reads. If not given, reads are discarded. Must be used with --contam_out_2')
subparser_remove_contam.add_argument('--contam_out_2', help='Name of output file 2 of contamination reads. If not given, reads are discarded. Must be used with --contam_out_1')
subparser_remove_contam.add_argument('--done_file', help='Write a file of the given name when the script is finished')
subparser_remove_contam.add_argument('metadata_tsv', help='Metadata TSV file. Format: one group of ref seqs per line. Tab-delimited columns: 1) group name; 2) 1|0 for is|is not contamination; 3+) sequence names.')
subparser_remove_contam.add_argument('bam_in', help='Name of input bam file')
subparser_remove_contam.add_argument('counts_out', help='Name of output file of read counts')
subparser_remove_contam.add_argument('reads_out_1', help='Name of output reads file 1')
subparser_remove_contam.add_argument('reads_out_2', help='Name of output reads file 2')
subparser_remove_contam.set_defaults(func=clockwork.tasks.remove_contam.run)


#---------------- remove_contam_make_jobs_tsv -----------------------
subparser_remove_contam_make_jobs_tsv = subparsers.add_parser(
    'remove_contam_make_jobs_tsv',
    help='Finds which sample need running remove_contam pipeline',
    usage='clockwork remove_contam_make_jobs_tsv <pipeline root> <db config file> <outfile> <reference ID> <references root>',
    description='Writes TSV file of remove_contam jobs to be run, and updates the database. TSV file is meant to be used as part of nextflow pipeline remove_contam',
)

subparser_remove_contam_make_jobs_tsv.add_argument('--dataset_name', help='limit to the given dataset name', metavar='STR')
subparser_remove_contam_make_jobs_tsv.add_argument('--pipeline_version', help='Pipeline version to run')
subparser_remove_contam_make_jobs_tsv.add_argument('pipeline_root', help='Root directory of pipeline')
subparser_remove_contam_make_jobs_tsv.add_argument('db_config_file', help='Name of database config file')
subparser_remove_contam_make_jobs_tsv.add_argument('outfile', help='Name of output file')
subparser_remove_contam_make_jobs_tsv.add_argument('reference_id', type=int, help='Reference genome database ID')
subparser_remove_contam_make_jobs_tsv.add_argument('reference_root', help='Root directory of pipeline reference genomes')
subparser_remove_contam_make_jobs_tsv.set_defaults(func=clockwork.tasks.remove_contam_make_jobs_tsv.run)


#---------------------------- samtools_qc ----------------------------
subparser_samtools_qc = subparsers.add_parser(
    'samtools_qc',
    help='QCs reads using samtools',
    usage='clockwork samtools_qc <reference fasta> <reads 1> <reads 2> <output directory>',
    description='Runs BWA MEM, picard MarkDuplicates, then samtools stats and plots',
)

subparser_samtools_qc.add_argument('ref_fasta', help='Name of reference fasta file. Must have had bwa index run on it')
subparser_samtools_qc.add_argument('reads1', help='Name of reads file 1')
subparser_samtools_qc.add_argument('reads2', help='Name of reads file 2')
subparser_samtools_qc.add_argument('output_dir', help='Name of output directory (must not already exist)')
subparser_samtools_qc.set_defaults(func=clockwork.tasks.samtools_qc.run)


#------------------------ trim reads --------------------------------
subparser_trim_reads = subparsers.add_parser(
    'trim_reads',
    help='Trim reads using Trimmomatic',
    usage='clockwork trim_reads <outprefix> <fwd reads 1> <ref reads 1> [<fwd reads 2> <rev reads 2> ...]',
    description='Adapter and quality trims N pairs of reads files using Trimmomatic',
)

subparser_trim_reads.add_argument('--trimmo_qual', help='Trimmomatic options used to quality trim reads [%(default)s]', default='LEADING:10 TRAILING:10 SLIDINGWINDOW:4:15', metavar='STRING')
subparser_trim_reads.add_argument('outprefix', help='Prefix of output files')
subparser_trim_reads.add_argument('reads_files', nargs='+', help='List of pairs of fwd, rev reads filenames. Must be even number of files!')
subparser_trim_reads.set_defaults(func=clockwork.tasks.trim_reads.run)


#---------------- variant_call_make_jobs_tsv -----------------------
subparser_variant_call_make_jobs_tsv = subparsers.add_parser(
    'variant_call_make_jobs_tsv',
    help='Finds which sample need running variant_call pipeline',
    usage='clockwork variant_call_make_jobs_tsv <pipeline root> <db config file> <outfile> <reference ID> <references root>',
    description='Writes TSV file of variant_call jobs to be run, and updates the database. TSV file is meant to be used as part of nextflow pipeline variant_call',
)

subparser_variant_call_make_jobs_tsv.add_argument('--dataset_name', help='limit to the given dataset name', metavar='STR')
subparser_variant_call_make_jobs_tsv.add_argument('--pipeline_version', help='Pipeline version to run')
subparser_variant_call_make_jobs_tsv.add_argument('pipeline_root', help='Root directory of pipeline')
subparser_variant_call_make_jobs_tsv.add_argument('db_config_file', help='Name of database config file')
subparser_variant_call_make_jobs_tsv.add_argument('outfile', help='Name of output file')
subparser_variant_call_make_jobs_tsv.add_argument('reference_id', type=int, help='Reference genome database ID')
subparser_variant_call_make_jobs_tsv.add_argument('reference_root', help='Root directory of pipeline reference genomes')
subparser_variant_call_make_jobs_tsv.set_defaults(func=clockwork.tasks.variant_call_make_jobs_tsv.run)


#----------------------------- version -------------------------------
subparser_version = subparsers.add_parser(
    'version',
    help='Get version and exit',
    usage='clockwork version',
    description='Print version and exit',
)
subparser_version.set_defaults(func=clockwork.tasks.version.run)

args = parser.parse_args()

if hasattr(args, 'func'):
    args.func(args)
else:
    parser.print_help()

